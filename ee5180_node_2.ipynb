{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0y-jubQY3Ge"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu_GhcSuhoPU",
        "outputId": "6514b55a-f871-4c9e-a0e4-52a732cce0c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/GANESH/node"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ5FU34Tg2QU",
        "outputId": "25448b7d-7b5e-4899-8dc4-aafc45d6b2b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GANESH/node\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "-jlSzT2NODbu",
        "outputId": "318ec534-ebd5-4609-d9ee-386d25780fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting https://github.com/facebookresearch/qhoptim/archive/master.zip (from -r requirements.txt (line 12))\n",
            "  Downloading https://github.com/facebookresearch/qhoptim/archive/master.zip\n",
            "\u001b[K     / 6.5 MB 4.6 MB/s\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=0.13 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (1.0.2)\n",
            "Collecting catboost==0.12.2\n",
            "  Downloading catboost-0.12.2-cp37-none-manylinux1_x86_64.whl (55.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 55.4 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting xgboost==0.81\n",
            "  Downloading xgboost-0.81-py2.py3-none-manylinux1_x86_64.whl (16.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.6 MB 24.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (4.64.0)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.3.5)\n",
            "Requirement already satisfied: prefetch_generator in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.0.1)\n",
            "Collecting enum34\n",
            "  Downloading enum34-1.1.10-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost==0.12.2->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.17->-r requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 10)) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 10)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 7)) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX->-r requirements.txt (line 9)) (3.17.3)\n",
            "Building wheels for collected packages: qhoptim\n",
            "  Building wheel for qhoptim (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for qhoptim: filename=qhoptim-1.1.0-py3-none-any.whl size=20333 sha256=55a10c23df1692e85b4117ced5178aab4be974f7431a50dc5e577758724bdd1f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-091x6eim/wheels/4e/8d/ab/172397a202926a7ebf44245fdb7e8c91fb85d3108fb0b556f9\n",
            "Successfully built qhoptim\n",
            "Installing collected packages: enum34, xgboost, tensorboardX, qhoptim, catboost\n",
            "  Attempting uninstall: xgboost\n",
            "    Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed catboost-0.12.2 enum34-1.1.10 qhoptim-1.1.0 tensorboardX-2.5 xgboost-0.81\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "enum"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5FHvzMNyLyM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e121891-e6d9-4689-b749-97e2abeda1dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=4bdcbb612898af6bf2aa44f2897474b99d115a1f73aed8d876634dceda96d3f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "import gzip\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Os9Z9ItyyOi"
      },
      "outputs": [],
      "source": [
        "# Dataset size\n",
        "# N_TRAIN_SAMPLES = 309871\n",
        "N_VAL_SAMPLES = 154937\n",
        "N_TEST_SAMPLES = 116203\n",
        "NUM_FEATURES = 54\n",
        "NUM_CLASSES = 7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCdVg4CXzQIN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc2af69-a9c0-4622-e4f9-e9380d3e9746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.4.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▉                            | 10 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 20 kB 29.8 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 30 kB 22.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 40 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 51 kB 11.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 61 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 71 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 81 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 86 kB 4.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.4.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.10.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.0.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (0.5.2)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from category_encoders) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.1->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.5.1->category_encoders) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.4.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import os\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import gzip\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import hyperopt\n",
        "from hyperopt import hp\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "from hyperopt import fmin, tpe, Trials, STATUS_OK, STATUS_FAIL\n",
        "from hyperopt.pyll.base import scope\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "\n",
        "import time\n",
        "!pip install category_encoders\n",
        "from category_encoders import LeaveOneOutEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import gzip\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from qhoptim.pyt import QHAdam\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from copy import deepcopy\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "from scipy import interp\n",
        "\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.preprocessing import LabelBinarizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RB96YELzf9G"
      },
      "outputs": [],
      "source": [
        "train=pd.read_csv('/content/drive/MyDrive/GANESH/data/train_forest_cover.csv')\n",
        "test=pd.read_csv('/content/drive/MyDrive/GANESH/data/test_forest_cover.csv')\n",
        "#val=pd.read_csv('/content/drive/MyDrive/ee5180/node/data/val_covertype.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=train.drop(labels=['Covertype'],axis=1)\n",
        "y=train['Covertype']"
      ],
      "metadata": {
        "id": "OLRNajWvuMPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test=test.drop(labels=['Covertype'],axis=1)\n",
        "y_test=test['Covertype']"
      ],
      "metadata": {
        "id": "WJgfmsVprmkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_val,y_train,y_val=train_test_split(X,y,train_size=0.8)"
      ],
      "metadata": {
        "id": "ngFBYc07ukFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x_train=x_train.to_numpy()\n",
        "y_train=y_train.to_numpy()\n",
        "x_val=x_val.to_numpy()\n",
        "y_val=y_val.to_numpy()\n",
        "x_test=x_test.to_numpy()\n",
        "y_test=y_test.to_numpy()\n",
        "x_train = x_train.astype('float32')\n",
        "x_val = x_val.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "set(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cpG3saXDC_m",
        "outputId": "ff448b9c-b6fa-48e3-bd35-c8d818427f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1, 2, 3, 4, 5, 6, 7}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "\n",
        "print(train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny4JYRTzDkNL",
        "outputId": "5ac1802f-2438-424f-aba0-ff8fd2be0ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(371847, 54)\n",
            "(371847,)\n",
            "(92962, 54)\n",
            "(92962,)\n",
            "(116203, 54)\n",
            "(116203,)\n",
            "(464809, 55)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZNO4JpNolth",
        "outputId": "0304bf3b-3f30-466d-f4e1-2bb951d5f82f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1, 2, 3, 4, 5, 6, 7}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y_train)):\n",
        "  temp = y_train[i] \n",
        "  y_train[i] = temp - 1\n",
        "set(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "von6pEq0jW3L",
        "outputId": "8a6c2bf0-8d30-4ace-bc17-e3c6739cd8f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y_val)):\n",
        "  temp = y_val[i] \n",
        "  y_val[i] = temp - 1\n",
        "set(y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_42tT2EHjvWF",
        "outputId": "2c18d99a-1749-4f72-91c5-b4dc1c7d6bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGoNOgme1dZd",
        "outputId": "f5eba905-9fff-40dc-b61d-f7de938c5535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y_val)):\n",
        "  temp = y_test[i] \n",
        "  y_test[i] = temp - 1\n",
        "set(y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1OQoqixj5CE",
        "outputId": "5802e93b-a024-4cf3-e9fb-cee47887390b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "dSg1K38x1Dhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGJxnQYB2MiY",
        "outputId": "7064d5be-f53f-419e-b6c0-9af2925ad515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2991.,   85.,    9., ...,    0.,    0.,    0.],\n",
              "       [3360.,  185.,   13., ...,    0.,    0.,    0.],\n",
              "       [3286.,  352.,    8., ...,    0.,    0.,    0.],\n",
              "       ...,\n",
              "       [3023.,  352.,    5., ...,    0.,    0.,    0.],\n",
              "       [2858.,  248.,    5., ...,    0.,    0.,    0.],\n",
              "       [3192.,  105.,   11., ...,    0.,    0.,    0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.choice(range(len(x_train)),10000)"
      ],
      "metadata": {
        "id": "EJgaXpap1GwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train[x]\n",
        "y_train = y_train[x]"
      ],
      "metadata": {
        "id": "FlIrQCAE2j6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.choice(range(len(x_val)),2500)"
      ],
      "metadata": {
        "id": "DQ4QH5r02o9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_val[x]\n",
        "y_val = y_val[x]"
      ],
      "metadata": {
        "id": "o7TTKm-03JZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "\n",
        "print(train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urU_fiLb3lVf",
        "outputId": "b336e9f5-f8aa-41b2-ee0c-fefc00739a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 54)\n",
            "(10000,)\n",
            "(2500, 54)\n",
            "(2500,)\n",
            "(116203, 54)\n",
            "(116203,)\n",
            "(464809, 55)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lib"
      ],
      "metadata": {
        "id": "8t3Vzta4TD4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "#device = 'cpu'\n",
        "\n",
        "print(f'Using {device}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggom7D5zfVyW",
        "outputId": "39d421fb-3bd8-4acb-af20-9a3e32cae6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CUDA_LAUNCH_BLOCKING=1"
      ],
      "metadata": {
        "id": "jPgcR7YrgDkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts = math.floor(time.time())\n",
        "experiment_name = f'node_forest_{ts}'"
      ],
      "metadata": {
        "id": "rWyOt3Ioa0hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    lib.DenseBlock(NUM_FEATURES,\n",
        "                   layer_dim=256,\n",
        "                   num_layers=4,\n",
        "                   tree_dim=NUM_CLASSES,\n",
        "                    depth=6,\n",
        "                   flatten_output=False,\n",
        "                   choice_function=lib.entmax15,\n",
        "                   bin_function=lib.entmoid15),\n",
        "    lib.Lambda(lambda x: x[..., :NUM_CLASSES].mean(dim=-2)),\n",
        ").to(device)\n",
        "with torch.no_grad():\n",
        "    \n",
        "    res = model(torch.as_tensor( (x_train[:2000]), device=device))\n",
        "    # trigger data-aware init\n",
        "#From the original code \n",
        "#find it at one of the notebooks in  /content/drive/......./node/notebooks"
      ],
      "metadata": {
        "id": "bl-eGPIQYqO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = lib.Trainer(\n",
        "    model=model, loss_function=F.cross_entropy,\n",
        "    experiment_name=experiment_name,\n",
        "    learning_rate=math.exp(-4),\n",
        "    warm_start=False,\n",
        "    total_tree_count = 2048,\n",
        "    Optimizer=QHAdam,\n",
        "    optimizer_params=dict(nus=(0.7, 1.0), betas=(0.95, 0.998)),\n",
        "    verbose=True,\n",
        "    n_last_checkpoints=5\n",
        ")"
      ],
      "metadata": {
        "id": "I5mAfKz2Zfh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_history, err_history = [], []\n",
        "best_val_err = 1.0\n",
        "best_step = 0\n",
        "early_stopping_rounds = 2500\n",
        "report_frequency = 1000"
      ],
      "metadata": {
        "id": "GyPo88d-ZiQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textfile1 = open(\"loss_hist2.txt\", \"a\")\n",
        "textfile1.write(\"ganesh\" + \"\\n\")\n",
        "textfile1.close()\n",
        "count = 1 "
      ],
      "metadata": {
        "id": "sXDjLO55QisO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textfile2 = open(\"err_his2.txt\", \"a\")\n",
        "textfile2.write(\"gaensh\" + \"\\n\")\n",
        "textfile2.close()"
      ],
      "metadata": {
        "id": "iOMTFDEmFlMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for batch in lib.iterate_minibatches(x_train,\n",
        "                                     y_train,\n",
        "                                     batch_size=512, \n",
        "                                     shuffle=True,\n",
        "                                     epochs=float('inf')):\n",
        "    metrics = trainer.train_on_batch(*batch, device=device)\n",
        "    \n",
        "    loss_history.append(metrics['loss'])\n",
        "    count = (count + 1)\n",
        "    count = count%200 \n",
        "    if(count==0):\n",
        "      textfile1 = open(\"loss_hist2.txt\", \"a\")\n",
        "      textfile1.write(str(metrics['loss']) + \"\\n\")\n",
        "      textfile1.close()\n",
        "    if trainer.step % report_frequency == 0:\n",
        "        trainer.save_checkpoint()\n",
        "        trainer.average_checkpoints(out_tag='avg')\n",
        "        trainer.load_checkpoint(tag='avg')\n",
        "        err = trainer.evaluate_classification_error(\n",
        "            x_val,\n",
        "            y_val,\n",
        "            device=device,\n",
        "            batch_size=128)\n",
        "        \n",
        "        if err < best_val_err:\n",
        "            best_val_err = err\n",
        "            best_step = trainer.step\n",
        "            trainer.save_checkpoint(tag='best')\n",
        "        \n",
        "        err_history.append(err)\n",
        "        textfile2 = open(\"err_his2.txt\", \"a\")\n",
        "        textfile2.write(str(err) + \"\\n\")\n",
        "        textfile2.close()\n",
        "        trainer.load_checkpoint()  # last\n",
        "        trainer.remove_old_temp_checkpoints()\n",
        "            \n",
        "        clear_output(True)\n",
        "        \n",
        "        print(\"Loss %.5f\" % (metrics['loss']))\n",
        "        print(\"Val Error Rate: %0.5f\" % (err))\n",
        "        \n",
        "    if trainer.step > best_step + early_stopping_rounds:\n",
        "        print('BREAK. There is no improvement for {} steps'.format(early_stopping_rounds))\n",
        "        print(\"Best step: \", best_step)\n",
        "        print(\"Best Val Error Rate: %0.5f\" % (best_val_err))\n",
        "        \n",
        "        \n",
        "        break"
      ],
      "metadata": {
        "id": "oxjjO1sCZkf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb24cba0-0b96-42e7-ab94-a49b6b2bc993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 0.55014\n",
            "Val Error Rate: 0.28560\n",
            "BREAK. There is no improvement for 2500 steps\n",
            "Best step:  3000\n",
            "Best Val Error Rate: 0.28400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "textfile = open(\"err3.txt\", \"w\")\n",
        "for element in err_history:\n",
        "  textfile. write(str(element) + \"\\n\")\n",
        "textfile. close()\n",
        "textfile = open(\"loas3.txt\", \"w\")\n",
        "for element in err_history:\n",
        "  textfile. write(str(element) + \"\\n\")\n",
        "textfile. close()"
      ],
      "metadata": {
        "id": "CyvQ8y9cbhq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def opt_fn(search_space):\n",
        "  print(search_space)\n",
        "  best_val_err = 1.0\n",
        "  best_step = 0\n",
        "  early_stopping_rounds = 2500\n",
        "  report_frequency = 1000\n",
        "  experiment_name = 'debug' # this bypasses some code that will be executed each time otherwise\n",
        "  \n",
        "  model = nn.Sequential(\n",
        "      lib.DenseBlock(NUM_FEATURES,\n",
        "                     layer_dim=128,\n",
        "                     num_layers=int(search_space['num_layers']),\n",
        "                     tree_dim=NUM_CLASSES,\n",
        "                     flatten_output=False,\n",
        "                     depth=int(search_space['depth']),\n",
        "                     choice_function=lib.entmax15,\n",
        "                     bin_function=lib.entmoid15),\n",
        "      lib.Lambda(lambda x: x[..., :NUM_CLASSES].mean(dim=-2)),\n",
        "    ).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    res = model(torch.as_tensor(x_train[:2000], device=device))\n",
        "    # trigger data-aware init\n",
        "\n",
        "  trainer = lib.Trainer(\n",
        "    model=model, loss_function=F.cross_entropy,\n",
        "    experiment_name=experiment_name,\n",
        "    warm_start=False,\n",
        "    total_tree_count = search_space['total_tree_count'] ,\n",
        "    learning_rate=search_space['learning_rate'],\n",
        "    Optimizer=QHAdam,\n",
        "    optimizer_params=dict(nus=(0.7, 1.0), betas=(0.95, 0.998)),\n",
        "    verbose=True,\n",
        "    n_last_checkpoints=5\n",
        "  )\n",
        "\n",
        "  for batch in lib.iterate_minibatches(x_train,\n",
        "                                     y_train,\n",
        "                                     batch_size=512, \n",
        "                                     shuffle=True,\n",
        "                                     epochs=float('inf')):\n",
        "    \n",
        "    metrics = trainer.train_on_batch(*batch, device=device)\n",
        "    \n",
        "\n",
        "    if trainer.step % report_frequency == 0:\n",
        "          err = trainer.evaluate_classification_error(\n",
        "              x_val,\n",
        "              y_val,\n",
        "              device=device,\n",
        "              batch_size=128)\n",
        "        \n",
        "          if err < best_val_err:\n",
        "              best_val_err = err\n",
        "              best_step = trainer.step\n",
        "              opt_metric = best_val_err\n",
        "          print(\"Loss %.5f\" % (metrics['loss']))\n",
        "          print(\"Val Error Rate: %0.5f\" % (err))\n",
        "          \n",
        "    if trainer.step > best_step + early_stopping_rounds:\n",
        "          opt_metric = best_val_err\n",
        "          break\n",
        "          \n",
        "  return {'loss': opt_metric, 'status': STATUS_OK}"
      ],
      "metadata": {
        "id": "_EiCMwKgtLN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#low = math.exp(-5)\n",
        "#high = math.exp(-4)\n",
        "search_space = {\n",
        "                'num_layers': hp.choice('num_layers', [2,4,8]),\n",
        "                'depth': hp.choice('depth',[6,8] ),\n",
        "                'learning_rate' : hp.loguniform('learning_rate',-5,0),\n",
        "                'total_tree_count' : hp.choice('total_tree_count', [1024,2048]),\n",
        "    }\n",
        "#'layer_dim': hp.choice('layer_dim', [512,1024])"
      ],
      "metadata": {
        "id": "_P-dkhCtXZfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3kUlHtbBlpOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#best = fmin(fn=opt_fn, \n",
        "  #          space=search_space, \n",
        "   #        algo=tpe.suggest, \n",
        "      #      max_evals=50) \n",
        "trials = Trials()\n",
        "best = fmin(fn=opt_fn, \n",
        "            space=search_space, \n",
        "            algo=tpe.suggest, \n",
        "            max_evals=10 \n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SNlB1apUhuH",
        "outputId": "14be6479-0b77-4245-9203-26e0ad7cfb2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'depth': 6, 'learning_rate': 1.0106412086788192, 'num_layers': 8, 'total_tree_count': 1024}\n",
            "Loss 0.89098\n",
            "Val Error Rate: 0.28019\n",
            "Loss 0.77203\n",
            "Val Error Rate: 0.26876\n",
            "Loss 0.70759\n",
            "Val Error Rate: 0.26484\n",
            "Loss 0.64784\n",
            "Val Error Rate: 0.26205\n",
            "Loss 0.62565\n",
            "Val Error Rate: 0.26408\n",
            "Loss 0.61797\n",
            "Val Error Rate: 0.23882\n",
            "  0%|          | 0/10 [34:27<?, ?it/s, best loss: ?]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KOBKjzTG3-ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V3q9HEjus0vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics_calc(y_true, y_pred, classes):\n",
        "\n",
        "    y_true_label = label_binarize(y_true, classes=classes)\n",
        "\n",
        "    y_pred_arg = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    cnf_matrix = confusion_matrix(y_true, y_pred_arg)\n",
        "\n",
        "    fp = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)  \n",
        "    fn = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "    tp = np.diag(cnf_matrix)\n",
        "    tn = cnf_matrix.sum() - (fp + fn + tp)\n",
        "\n",
        "    fp = fp.astype(float)\n",
        "    fn = fn.astype(float)\n",
        "    tp = tp.astype(float)\n",
        "    tn = tn.astype(float)\n",
        "\n",
        "    tpr = tp/(tp+fn)\n",
        "    fpr = fp/(fp+tn)\n",
        "    precision = tp/(tp+fp)\n",
        "    acc = (tp+tn)/(tp+fp+fn+tn)\n",
        "\n",
        "    mean_acc = np.nanmean(acc)\n",
        "    mean_tpr = np.nanmean(tpr)\n",
        "    mean_fpr = np.nanmean(fpr)\n",
        "    mean_precision = np.nanmean(precision)\n",
        "\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc_list = []\n",
        "    pr_auc_list = []\n",
        "\n",
        "    if len(classes) == 2:\n",
        "        fpr, tpr, threshold_roc = roc_curve(y_true, y_pred[:, 1])\n",
        "\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        roc_auc_list.append(roc_auc)\n",
        "        \n",
        "        precision, recall, threshold_pr = precision_recall_curve(y_true,\n",
        "                                                            y_pred[:, 1])\n",
        "        \n",
        "        pr_auc = auc(recall, precision)\n",
        "        pr_auc_list.append(pr_auc)\n",
        "\n",
        "    else:\n",
        "        for i in range(y_pred.shape[1]):\n",
        "            fpr[i], tpr[i], threshold_roc = roc_curve(y_true_label[:, i], y_pred[:, i])\n",
        "            # roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "            roc_auc = auc(fpr[i], tpr[i])\n",
        "            roc_auc_list.append(roc_auc)\n",
        "            \n",
        "            precision[i], recall[i], threshold_pr = precision_recall_curve(y_true_label[:, i],\n",
        "                                                                y_pred[:, i])\n",
        "\n",
        "            pr_auc = auc(recall[i], precision[i])\n",
        "            pr_auc_list.append(pr_auc)\n",
        "\n",
        "    mean_roc_auc = np.nanmean(roc_auc_list)\n",
        "    mean_pr_auc = np.nanmean(pr_auc_list)\n",
        "\n",
        "    print('mean roc auc',mean_roc_auc)\n",
        "    print('mean pr auc', mean_pr_auc)\n",
        "\n",
        "    return mean_acc, mean_tpr, mean_fpr, mean_precision, mean_roc_auc, mean_pr_auc\n",
        "\n",
        "def prepare_data(df, target_col=None):\n",
        "    df = df.fillna(df.mean())\n",
        "    if target_col:\n",
        "        X = df.iloc[:, df.columns != target_col]\n",
        "        y = df.loc[:, df.columns == target_col]\n",
        "    else:\n",
        "        X = df.iloc[:, :-1]\n",
        "        y = df.iloc[:, -1]\n",
        "    lb = LabelEncoder()\n",
        "    y = lb.fit_transform(y)\n",
        "    X = dummy_encode(X)\n",
        "    classes = np.unique(y)\n",
        "\n",
        "    return X, y, classes\n",
        "\n",
        "def dummy_encode(df):\n",
        "    cols_to_encode = list(df.select_dtypes(include=['category','object']))\n",
        "    if len(cols_to_encode): \n",
        "        df = pd.get_dummies(df, columns = cols_to_encode, prefix=cols_to_encode)\n",
        "\n",
        "    return df.values\n"
      ],
      "metadata": {
        "id": "JcdH860isaKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = list(set(y_val))"
      ],
      "metadata": {
        "id": "ZqB_PBBiK1-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, classes = prepare_data(train)"
      ],
      "metadata": {
        "id": "MTbBu5kULCuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XI8-UBaZLubz",
        "outputId": "2a9985b5-6626-48b6-8176-c4b7c4b7b520"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_proba(self, X_test, y_test, device, batch_size=32):\n",
        "    X_test = torch.as_tensor(X_test, device=device)\n",
        "    y_test = lib.utils.check_numpy(y_test)\n",
        "    self.model.train(False)\n",
        "    with torch.no_grad():\n",
        "        logits = F.softmax(lib.utils.process_in_chunks(self.model, X_test, batch_size=batch_size), dim=1)\n",
        "        logits = lib.utils.check_numpy(logits)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "t4Sy0_M_sg0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lib.Trainer.predict_proba = predict_proba"
      ],
      "metadata": {
        "id": "9aJlmPJZskIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_QbgHRkNrQ7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "968e82f7-7b0a-4ba3-86ba-b7b8c2d783d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean roc auc 0.9648405124569777\n",
            "mean pr auc 0.8441325175767507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t2 = time.time()\n",
        "y_pred_prob = trainer.predict_proba(x_test, y_test ,device=device, batch_size=128)\n",
        "t3 = time.time()\n",
        "inference_time = (t3 - t2) * 1000 / y_val.shape[0]\n",
        "acc_list = []\n",
        "tpr_list = []\n",
        "fpr_list = []\n",
        "precision_list = []\n",
        "roc_auc_list = []\n",
        "pr_auc_list = []\n",
        "training_time_list = []\n",
        "inference_time_list = []\n",
        "cross_val = []\n",
        "\n",
        "#acc, mean_tpr, mean_fpr, mean_precision, mean_roc_auc, mean_pr_auc = metrics_calc(y_test, y_pred_prob, classes)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VbEPRGj0n5x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_test)"
      ],
      "metadata": {
        "id": "fdfXcF6yMAqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "loss1 = criterion(torch.from_numpy(trainer.predict_proba(x_test)),torch.from_numpy(y_test))\n",
        "print(loss1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "YBF-mmOgpdws",
        "outputId": "d4438b67-9e7e-4b39-e656-e4210cfa501f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-d9d8651aebf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'predict_proba'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss1 = trainer.loss_function(model(torch.as_tensor((x_test[:2000]),device=device)),torch.as_tensor(y_test[:2000], device=device)).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "OT98ZdYVppO7",
        "outputId": "cdd1b266-5b8c-4db2-d97e-fc8a5be65a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-a467ddf651ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "zQU3xKEB47z_",
        "outputId": "69d33768-fbe9-47b4-f157-d9ce5dbfdb6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 699\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    396\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                                 \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_default_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_safe_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mklass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_baseclass_reprs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;31m# A user-provided repr. Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'<'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_line\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# All strings are unicode in Python 3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_str_intern\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str_intern\u001b[0;34m(inp)\u001b[0m\n\u001b[1;32m    388\u001b[0m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                     \u001b[0mtensor_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_formatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimag_formatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mnonzero_finite_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_view\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnonzero_finite_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss1 = trainer.loss_function(model(torch.as_tensor( (x_val), device=device)),torch.as_tensor(y_val, device=device)).mean()"
      ],
      "metadata": {
        "id": "QKS2nw_I8SB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss1"
      ],
      "metadata": {
        "id": "q_fWIFLK8YJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_batch = torch.as_tensor(y_test, device=device)\n",
        "y_batch = torch.as_tensor(y_pred_prob, device=device)\n"
      ],
      "metadata": {
        "id": "AfI4Gp4GqATI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "r7Rs1bLYpQMz",
        "outputId": "d9825818-88d3-465f-cdd6-dde3777db56a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-522d22ba1f59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1178\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Trainer' object has no attribute 'predict'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(y_pred_prob).to_csv(\"data_preds_prob.csv\")"
      ],
      "metadata": {
        "id": "uD-SKg74o9Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUU6eKo6dNnP"
      },
      "outputs": [],
      "source": [
        "\"\"\"a1=hyperopt.pyll.stochastic.sample(reg_params)\n",
        "print(a1)\n",
        "tb_ll_cv(a1)\n",
        "#torch.from_numpy(a1['gamma']).dtype\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "izZAz63EW9tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#params need to be replaced with with best params \n",
        "model = nn.Sequential(\n",
        "      lib.DenseBlock(NUM_FEATURES,\n",
        "                     layer_dim=128,\n",
        "                     num_layers=int(search_space['num_layers']),\n",
        "                     tree_dim=NUM_CLASSES,\n",
        "                     flatten_output=False,\n",
        "                     depth=int(search_space['depth']),\n",
        "                     choice_function=lib.entmax15,\n",
        "                     bin_function=lib.entmoid15),\n",
        "      lib.Lambda(lambda x: x[..., :NUM_CLASSES].mean(dim=-2)),\n",
        "    ).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    res = model(torch.as_tensor(x_train[:2000], device=device))\n",
        "    # trigger data-aware init\n",
        "trainer = lib.Trainer(\n",
        "    model=model, loss_function=F.cross_entropy,\n",
        "    experiment_name=experiment_name,\n",
        "    warm_start=False,\n",
        "    total_tree_count = search_space['total_tree_count'] ,\n",
        "    learning_rate=search_space['learning_rate'],\n",
        "    Optimizer=QHAdam,\n",
        "    optimizer_params=dict(nus=(0.7, 1.0), betas=(0.95, 0.998)),\n",
        "    verbose=True,\n",
        "    n_last_checkpoints=5\n",
        "  )\n",
        "\n",
        "loss_history, err_history = [], []\n",
        "best_val_err = 1.0\n",
        "best_step = 0\n",
        "early_stopping_rounds = 2500\n",
        "report_frequency = 1000\n",
        "for batch in lib.iterate_minibatches(x_train,\n",
        "                                     y_train,\n",
        "                                     batch_size=512, \n",
        "                                     shuffle=True,\n",
        "                                     epochs=float('inf')):\n",
        "    metrics = trainer.train_on_batch(*batch, device=device)\n",
        "    \n",
        "    loss_history.append(metrics['loss'])\n",
        "\n",
        "    if trainer.step % report_frequency == 0:\n",
        "        trainer.save_checkpoint()\n",
        "        trainer.average_checkpoints(out_tag='avg')\n",
        "        trainer.load_checkpoint(tag='avg')\n",
        "        err = trainer.evaluate_classification_error(\n",
        "            x_val,\n",
        "            y_val,\n",
        "            device=device,\n",
        "            batch_size=128)\n",
        "        \n",
        "        if err < best_val_err:\n",
        "            best_val_err = err\n",
        "            best_step = trainer.step\n",
        "            trainer.save_checkpoint(tag='best')\n",
        "        \n",
        "        err_history.append(err)\n",
        "        trainer.load_checkpoint()  # last\n",
        "        trainer.remove_old_temp_checkpoints()\n",
        "            \n",
        "        clear_output(True)\n",
        "        \n",
        "        print(\"Loss %.5f\" % (metrics['loss']))\n",
        "        print(\"Val Error Rate: %0.5f\" % (err))\n",
        "        \n",
        "    if trainer.step > best_step + early_stopping_rounds:\n",
        "        print('BREAK. There is no improvement for {} steps'.format(early_stopping_rounds))\n",
        "        print(\"Best step: \", best_step)\n",
        "        print(\"Best Val Error Rate: %0.5f\" % (best_val_err))\n",
        "        break"
      ],
      "metadata": {
        "id": "wxRHf7-3W-SH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ee5180_node_2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}